# -*- coding: utf-8 -*-
"""Sentimen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nB3FEKxM-s7oUnnIhp1H4vVi6aqBhka0
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc
import pickle
from wordcloud import WordCloud
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

df = pd.read_csv("/content/sentiment.csv", encoding='ISO-8859-1')

print("Shape:", df.shape)
print(df.head())

# Check for missing values
print("\nMissing Values:\n", df.isnull().sum())

# Sentiment distribution
sns.countplot(data=df, x='sentiment')
plt.title("Sentiment Distribution")
plt.show()

!pip install emoji
!pip install contractions
import emoji
import contractions

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = text.lower()
    text = emoji.replace_emoji(text, replace='')
    text = contractions.fix(text)
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"[^a-z\s]", "", text)
    words = text.split()
    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]
    return " ".join(words)

df['clean_text'] = df['text'].astype(str).apply(clean_text)

print(df['sentiment'].unique())

sentiments = df['sentiment'].unique()
for sentiment in sentiments:
    text = " ".join(df[df['sentiment'] == sentiment]['clean_text'])

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(f"WordCloud for {sentiment}")
    plt.axis('off')
    plt.show()

df['label'] = df['sentiment'].astype('category').cat.codes
X = df['clean_text']
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

tfidf = TfidfVectorizer(max_features=7000, ngram_range=(1, 2))
X_train_vec = tfidf.fit_transform(X_train)
X_test_vec = tfidf.transform(X_test)

log_model = LogisticRegression(max_iter=200)
log_model.fit(X_train_vec, y_train)

svm_model = SVC(probability=True)
svm_model.fit(X_train_vec, y_train)

def evaluate(model, X_test, y_test):
    y_pred = model.predict(X_test)
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))

    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

evaluate(log_model, X_test_vec, y_test)
evaluate(svm_model, X_test_vec, y_test)

y_score = log_model.predict_proba(X_test_vec)
fpr = dict()
tpr = dict()
roc_auc = dict()

for i in np.unique(y_test):
    fpr[i], tpr[i], _ = roc_curve((y_test == i).astype(int), y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    plt.plot(fpr[i], tpr[i], label=f"Class {i} (AUC = {roc_auc[i]:.2f})")

plt.plot([0, 1], [0, 1], 'k--')
plt.title("ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc='best')
plt.show()

with open("logistic_model.pkl", "wb") as f:
    pickle.dump(log_model, f)
with open("tfidf_vectorizer.pkl", "wb") as f:
    pickle.dump(tfidf, f)

def predict_sentiment(text):
    cleaned = clean_text(text)
    vec = tfidf.transform([cleaned])
    pred = log_model.predict(vec)
    label = df['sentiment'].astype('category').cat.categories[pred[0]]
    return label

sample_text = "I love this so much! Best experience ever!"
print("Predicted Sentiment:", predict_sentiment(sample_text))

from sklearn.model_selection import cross_val_score

log_scores = cross_val_score(log_model, X_train_vec, y_train, cv=5)
svm_scores = cross_val_score(svm_model, X_train_vec, y_train, cv=5)

print("Logistic Regression CV Accuracy:", np.mean(log_scores))
print("SVM CV Accuracy:", np.mean(svm_scores))

models = ['Logistic Regression', 'SVM']
scores = [np.mean(log_scores), np.mean(svm_scores)]

plt.bar(models, scores, color=['skyblue', 'orange'])
plt.ylabel('Cross-Validation Accuracy')
plt.title('Model Comparison')
plt.ylim(0, 1)
for i, v in enumerate(scores):
    plt.text(i, v + 0.01, f"{v:.2f}", ha='center', fontweight='bold')
plt.show()

def show_most_informative_features(vectorizer, model, n=20):
    feature_names = np.array(vectorizer.get_feature_names_out())
    coefs = model.coef_[0]
    top_positive = np.argsort(coefs)[-n:]
    top_negative = np.argsort(coefs)[:n]
    print("Top Positive Words:")
    print(feature_names[top_positive])
    print("Top Negative Words:")
    print(feature_names[top_negative])

show_most_informative_features(tfidf, log_model)

y_pred = log_model.predict(X_test_vec)
misclassified_indices = np.where(y_pred != y_test)[0]
print(f"Total Misclassifications: {len(misclassified_indices)}")

# Show a few misclassified examples
for i in misclassified_indices[:5]:
    print(f"\nText: {X_test.iloc[i]}")
    print(f"Actual: {df['sentiment'].astype('category').cat.categories[y_test.iloc[i]]}")
    print(f"Predicted: {df['sentiment'].astype('category').cat.categories[y_pred[i]]}")

misclassified_df = pd.DataFrame({
    "text": X_test.iloc[misclassified_indices],
    "actual": y_test.iloc[misclassified_indices].map(lambda x: df['sentiment'].astype('category').cat.categories[x]),
    "predicted": y_pred[misclassified_indices].astype(int)
})
misclassified_df['predicted'] = misclassified_df['predicted'].map(lambda x: df['sentiment'].astype('category').cat.categories[x])
misclassified_df.to_csv("misclassified_samples.csv", index=False)
print("Saved misclassified samples to 'misclassified_samples.csv'")

import joblib

joblib.dump(log_model, "logistic_model.joblib")
joblib.dump(svm_model, "svm_model.joblib")
joblib.dump(tfidf, "tfidf_vectorizer.joblib")
print("Models and vectorizer saved.")

